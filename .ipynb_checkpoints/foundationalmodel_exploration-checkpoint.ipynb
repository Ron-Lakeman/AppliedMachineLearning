{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a18909b-7808-4eff-afcd-1998eaaaaba4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "327a4248-8a00-4817-a6ad-870ca0250f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc08b52-dee1-4064-92a5-e762f40f5014",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3558e822-643d-4422-ba0a-c1404a435325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: C:\\Users\\d.dehaan\\OneDrive - degoedewoning\\Bureaublad\\School\\Applied machine learning\\AppliedMachineLearning\\Data\n",
      "Covertype train: Data\\covtype_train.csv\n",
      "HELOC train: Data\\heloc_train.csv\n",
      "HIGGS train: Data\\higgs_train.csv\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = \"Data\"  # Folder\n",
    "\n",
    "# Covertype\n",
    "COV_TRAIN = os.path.join(DATA_DIR, \"covtype_train.csv\")\n",
    "COV_TEST  = os.path.join(DATA_DIR, \"covtype_test.csv\")\n",
    "\n",
    "# HELOC\n",
    "HELOC_TRAIN = os.path.join(DATA_DIR, \"heloc_train.csv\")\n",
    "HELOC_TEST  = os.path.join(DATA_DIR, \"heloc_test.csv\")\n",
    "\n",
    "# HIGGS\n",
    "HIGGS_TRAIN = os.path.join(DATA_DIR, \"higgs_train.csv\")\n",
    "HIGGS_TEST  = os.path.join(DATA_DIR, \"higgs_test.csv\")\n",
    "\n",
    "# Sample submissions (optional)\n",
    "COV_SAMPLE_SUB   = os.path.join(DATA_DIR, \"covtype_test_submission.csv\")\n",
    "HELOC_SAMPLE_SUB = os.path.join(DATA_DIR, \"heloc_test_submission.csv\")\n",
    "HIGGS_SAMPLE_SUB = os.path.join(DATA_DIR, \"higgs_test_submission.csv\")\n",
    "\n",
    "print(\"Data directory:\", os.path.abspath(DATA_DIR))\n",
    "print(\"Covertype train:\", COV_TRAIN)\n",
    "print(\"HELOC train:\", HELOC_TRAIN)\n",
    "print(\"HIGGS train:\", HIGGS_TRAIN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e992831-6972-4b2b-a7ed-f83a7eec702c",
   "metadata": {},
   "source": [
    "# Load and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14aab1a1-ca0e-44d1-b4c6-bfc7c86316f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_covertype():\n",
    "    \"\"\"Load and preprocess the CoverType dataset.\n",
    "\n",
    "    Expects:\n",
    "    - training.csv with a column Cover_Type or CoverType as target.\n",
    "    - test.csv with the same feature columns (without target).\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(COV_TRAIN)\n",
    "    df_test  = pd.read_csv(COV_TEST)\n",
    "\n",
    "    # Target column can be 'Cover_Type' or 'CoverType'\n",
    "    if \"Cover_Type\" in df_train.columns:\n",
    "        y_col = \"Cover_Type\"\n",
    "    elif \"CoverType\" in df_train.columns:\n",
    "        y_col = \"CoverType\"\n",
    "    else:\n",
    "        raise ValueError(\"Could not find CoverType label column in covertype training.csv\")\n",
    "\n",
    "    y = df_train[y_col].values\n",
    "    X = df_train.drop(columns=[y_col])\n",
    "    X_test = df_test.copy()\n",
    "\n",
    "    # All numeric; scale with StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X.astype(np.float32))\n",
    "    X_test_scaled = scaler.transform(X_test.astype(np.float32))\n",
    "\n",
    "    return X_scaled, y, X_test_scaled\n",
    "\n",
    "\n",
    "def load_heloc():\n",
    "    \"\"\"Load and preprocess the HELOC dataset.\n",
    "\n",
    "    - training.csv with column 'RiskPerformance' (Good/Bad).\n",
    "    - test.csv with the same feature columns.\n",
    "    - Sentinel codes -7, -8, -9 are treated as missing and imputed.\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(HELOC_TRAIN)\n",
    "    df_test  = pd.read_csv(HELOC_TEST)\n",
    "\n",
    "    # Label: Good/Bad -> 0/1 (Bad = 1)\n",
    "    y = (df_train[\"RiskPerformance\"] == \"Bad\").astype(int).values\n",
    "    X = df_train.drop(columns=[\"RiskPerformance\"])\n",
    "    X_test = df_test.copy()\n",
    "\n",
    "    # Replace sentinel values with NaN\n",
    "    sentinel = [-7, -8, -9]\n",
    "    X = X.replace(sentinel, np.nan).astype(np.float32)\n",
    "    X_test = X_test.replace(sentinel, np.nan).astype(np.float32)\n",
    "\n",
    "    # Impute NaNs with train medians\n",
    "    medians = X.median()\n",
    "    X = X.fillna(medians)\n",
    "    X_test = X_test.fillna(medians)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_scaled, y, X_test_scaled\n",
    "\n",
    "\n",
    "def load_higgs():\n",
    "    \"\"\"Load and preprocess the HIGGS dataset.\n",
    "\n",
    "    - training.csv with columns: EventId, 30 features, Weight, Label (b/s or 0/1).\n",
    "    - test.csv with EventId and the 30 feature columns.\n",
    "\n",
    "    We treat -999.0 as missing and impute with medians.\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(HIGGS_TRAIN)\n",
    "    df_test  = pd.read_csv(HIGGS_TEST)\n",
    "\n",
    "    # Label may be given as 'Label' (b/s) or 'label' (0/1)\n",
    "    if \"Label\" in df_train.columns:\n",
    "        y_raw = df_train[\"Label\"]\n",
    "        y = (y_raw == \"s\").astype(int).values\n",
    "    elif \"label\" in df_train.columns:\n",
    "        y = df_train[\"label\"].astype(int).values\n",
    "    else:\n",
    "        raise ValueError(\"Could not find label column ('Label' or 'label') in HIGGS training.csv\")\n",
    "\n",
    "    # Sample weights (if available)\n",
    "    if \"Weight\" in df_train.columns:\n",
    "        w = df_train[\"Weight\"].values.astype(np.float32)\n",
    "    else:\n",
    "        w = np.ones(len(df_train), dtype=np.float32)\n",
    "\n",
    "    # Features: drop ID, Weight, label columns\n",
    "    drop_cols = [c for c in [\"EventId\", \"Weight\", \"Label\", \"label\"] if c in df_train.columns]\n",
    "    feature_cols = [c for c in df_train.columns if c not in drop_cols]\n",
    "    X = df_train[feature_cols].copy()\n",
    "    X_test = df_test[feature_cols].copy()\n",
    "\n",
    "    # Replace sentinel -999.0 with NaN\n",
    "    X = X.replace(-999.0, np.nan).astype(np.float32)\n",
    "    X_test = X_test.replace(-999.0, np.nan).astype(np.float32)\n",
    "\n",
    "    # Impute with medians\n",
    "    medians = X.median()\n",
    "    X = X.fillna(medians)\n",
    "    X_test = X_test.fillna(medians)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Event IDs (optional, useful for submissions)\n",
    "    event_id_train = df_train[\"EventId\"] if \"EventId\" in df_train.columns else None\n",
    "    event_id_test  = df_test[\"EventId\"] if \"EventId\" in df_test.columns else None\n",
    "\n",
    "    return X_scaled, y, w, X_test_scaled, event_id_train, event_id_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f3da34-0300-49b1-9c47-8ca086d457a9",
   "metadata": {},
   "source": [
    "# load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae97df4b-6641-4d0f-a251-7ed8a46531c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoverType: (58101, 54) (58101,)\n",
      "HELOC: (9413, 23) (9413,)\n",
      "HIGGS: (175000, 30) (175000,)\n"
     ]
    }
   ],
   "source": [
    "X_cov,   y_cov,   X_cov_test   = load_covertype()\n",
    "X_heloc, y_heloc, X_heloc_test = load_heloc()\n",
    "X_higgs, y_higgs, w_higgs, X_higgs_test, eid_tr, eid_te = load_higgs()\n",
    "\n",
    "print(\"CoverType:\", X_cov.shape, y_cov.shape)\n",
    "print(\"HELOC:\", X_heloc.shape, y_heloc.shape)\n",
    "print(\"HIGGS:\", X_higgs.shape, y_higgs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e56e926-be4c-4099-8360-c16c252aa71b",
   "metadata": {},
   "source": [
    "# Build a unified feature and label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab5d9d61-c162-4b96-bfde-51ec18104cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified X_all: (242514, 110) y_all: (242514,)\n"
     ]
    }
   ],
   "source": [
    "# Feature block sizes\n",
    "d_cov   = X_cov.shape[1]\n",
    "d_heloc = X_heloc.shape[1]\n",
    "d_higgs = X_higgs.shape[1]\n",
    "\n",
    "# Total unified feature length + 3 dataset-indicator features\n",
    "D_total = d_cov + d_heloc + d_higgs + 3\n",
    "\n",
    "def embed_block(X, dataset_idx):\n",
    "    \"\"\"\n",
    "    Embed X (n_samples, d_dataset) into unified feature space.\n",
    "\n",
    "    dataset_idx: 0 = CoverType, 1 = HELOC, 2 = HIGGS\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    Z = np.zeros((n, D_total), dtype=np.float32)\n",
    "\n",
    "    if dataset_idx == 0:          # CoverType block\n",
    "        Z[:, :d_cov] = X\n",
    "    elif dataset_idx == 1:        # HELOC block\n",
    "        Z[:, d_cov:d_cov + d_heloc] = X\n",
    "    elif dataset_idx == 2:        # HIGGS block\n",
    "        Z[:, d_cov + d_heloc:d_cov + d_heloc + d_higgs] = X\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset_idx, expected 0,1,2\")\n",
    "\n",
    "    # Dataset one-hot in last 3 positions\n",
    "    Z[:, D_total - 3 + dataset_idx] = 1.0\n",
    "\n",
    "    return Z\n",
    "\n",
    "\n",
    "# ----- Map labels into unified label space -----\n",
    "\n",
    "# CoverType: original labels (1..7) -> map to 0..6\n",
    "cov_unique = np.sort(np.unique(y_cov))\n",
    "cov_map = {v: i for i, v in enumerate(cov_unique)}\n",
    "y_cov_int = np.array([cov_map[v] for v in y_cov], dtype=np.int64)\n",
    "\n",
    "# HELOC: 0/1 -> shift by +7  (7, 8)\n",
    "y_heloc_int = y_heloc.astype(np.int64) + 7\n",
    "\n",
    "# HIGGS: 0/1 -> shift by +9  (9, 10)\n",
    "y_higgs_int = y_higgs.astype(np.int64) + 9\n",
    "\n",
    "# Embed features (train)\n",
    "X_cov_emb   = embed_block(X_cov,   dataset_idx=0)\n",
    "X_heloc_emb = embed_block(X_heloc, dataset_idx=1)\n",
    "X_higgs_emb = embed_block(X_higgs, dataset_idx=2)\n",
    "\n",
    "# Embed features (test) – needed for the submission step\n",
    "X_cov_test_emb   = embed_block(X_cov_test,   dataset_idx=0)\n",
    "X_heloc_test_emb = embed_block(X_heloc_test, dataset_idx=1)\n",
    "X_higgs_test_emb = embed_block(X_higgs_test, dataset_idx=2)\n",
    "\n",
    "# Concatenate everything (train)\n",
    "X_all = np.vstack([X_cov_emb, X_heloc_emb, X_higgs_emb])\n",
    "y_all = np.concatenate([y_cov_int, y_heloc_int, y_higgs_int])\n",
    "\n",
    "# Sample weights: CoverType & HELOC = 1, HIGGS uses its normalised weight\n",
    "w_cov   = np.ones_like(y_cov_int,   dtype=np.float32)\n",
    "w_heloc = np.ones_like(y_heloc_int, dtype=np.float32)\n",
    "w_h_norm = w_higgs.astype(np.float32) / np.mean(w_higgs)\n",
    "\n",
    "sample_weight_all = np.concatenate([w_cov, w_heloc, w_h_norm])\n",
    "\n",
    "print(\"Unified X_all:\", X_all.shape, \"y_all:\", y_all.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260707b2-4434-4425-a6c5-5e2b417d18a7",
   "metadata": {},
   "source": [
    "# Train/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f32e411b-6495-4c57-bc09-1e691b3e7a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (194011, 110) Val: (48503, 110)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val, w_train, w_val = train_test_split(\n",
    "    X_all,\n",
    "    y_all,\n",
    "    sample_weight_all,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=y_all,\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f34c51-f0f2-4d67-95a6-1c206002302d",
   "metadata": {},
   "source": [
    "# Train foundationmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "105744d4-620b-4771-b3cf-deda7bb7e038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.51957959\n",
      "Validation score: 0.810741\n",
      "Iteration 2, loss = 0.43057438\n",
      "Validation score: 0.813524\n",
      "Iteration 3, loss = 0.41642408\n",
      "Validation score: 0.816823\n",
      "Iteration 4, loss = 0.40530478\n",
      "Validation score: 0.822441\n",
      "Iteration 5, loss = 0.39701174\n",
      "Validation score: 0.823163\n",
      "Iteration 6, loss = 0.38906431\n",
      "Validation score: 0.828677\n",
      "Iteration 7, loss = 0.38257380\n",
      "Validation score: 0.824090\n",
      "Iteration 8, loss = 0.37571681\n",
      "Validation score: 0.825740\n",
      "Iteration 9, loss = 0.37006722\n",
      "Validation score: 0.830636\n",
      "Iteration 10, loss = 0.36523836\n",
      "Validation score: 0.830121\n",
      "Iteration 11, loss = 0.36002189\n",
      "Validation score: 0.831821\n",
      "Iteration 12, loss = 0.35544113\n",
      "Validation score: 0.835996\n",
      "Iteration 13, loss = 0.35074970\n",
      "Validation score: 0.834347\n",
      "Iteration 14, loss = 0.34603811\n",
      "Validation score: 0.832646\n",
      "Iteration 15, loss = 0.34217738\n",
      "Validation score: 0.835017\n",
      "Iteration 16, loss = 0.33861312\n",
      "Validation score: 0.834862\n",
      "Iteration 17, loss = 0.33438061\n",
      "Validation score: 0.834553\n",
      "Iteration 18, loss = 0.32995566\n",
      "Validation score: 0.833419\n",
      "Validation score did not improve more than tol=0.000100 for 5 consecutive epochs. Stopping.\n",
      "Finished training.\n",
      "Final training loss: 0.3299556606479452\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 256),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    alpha=1e-4,\n",
    "    batch_size=256,\n",
    "    learning_rate_init=1e-3,\n",
    "    max_iter=80,           # increase if really needed\n",
    "    early_stopping=True,\n",
    "    n_iter_no_change=5,\n",
    "    random_state=SEED,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Important: MLPClassifier does NOT support sample_weight,\n",
    "# so we do NOT pass w_train here.\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Finished training.\")\n",
    "if hasattr(clf, \"loss_curve_\"):\n",
    "    print(\"Final training loss:\", clf.loss_curve_[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855f620e-bb03-4b27-9060-f1cf3f444ec3",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8d5a40f-f95a-45bd-a499-fc359eeb58bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoverType accuracy (on provided data): 0.8511\n",
      "CoverType: 0.8510696889898625\n",
      "HELOC accuracy (on provided data): 0.7617\n",
      "HELOC:    0.7617125252310635\n",
      "HIGGS accuracy (on provided data): 0.8485\n",
      "HIGGS:    0.84848\n"
     ]
    }
   ],
   "source": [
    "def eval_dataset(X_emb, y_int, name):\n",
    "    y_pred_int = clf.predict(X_emb)\n",
    "    acc = accuracy_score(y_int, y_pred_int)\n",
    "    print(f\"{name} accuracy (on provided data): {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "print(\"CoverType:\", eval_dataset(X_cov_emb,   y_cov_int,   \"CoverType\"))\n",
    "print(\"HELOC:   \", eval_dataset(X_heloc_emb, y_heloc_int, \"HELOC\"))\n",
    "print(\"HIGGS:   \", eval_dataset(X_higgs_emb, y_higgs_int, \"HIGGS\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67c3b1-dfec-49e9-8feb-8ffd000dc0d3",
   "metadata": {},
   "source": [
    "# Generate submition file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90d32de7-4a01-4850-95ce-e23ae37ea537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating unified submission file...\n",
      "\n",
      "\n",
      "Submission preview:\n",
      "   ID  Prediction\n",
      "0   1           0\n",
      "1   2           0\n",
      "2   3           0\n",
      "3   4           0\n",
      "4   5           0\n",
      "          ID  Prediction\n",
      "79541  79542          10\n",
      "79542  79543           9\n",
      "79543  79544           9\n",
      "79544  79545           9\n",
      "79545  79546           9\n",
      "\n",
      "Total rows: 79546\n",
      "\n",
      "✔ Saved unified submission to: combined_submission.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating unified submission file...\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Embed test datasets\n",
    "# -----------------------------\n",
    "X_cov_test_emb  = embed_block(X_cov_test,  dataset_idx=0)\n",
    "X_heloc_test_emb = embed_block(X_heloc_test, dataset_idx=1)\n",
    "X_higgs_test_emb = embed_block(X_higgs_test, dataset_idx=2)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Predict each dataset\n",
    "# -----------------------------\n",
    "cov_pred   = clf.predict(X_cov_test_emb)\n",
    "heloc_pred = clf.predict(X_heloc_test_emb)\n",
    "higgs_pred = clf.predict(X_higgs_test_emb)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Assign correct ID ranges\n",
    "# -----------------------------\n",
    "# CoverType: IDs start at 1\n",
    "cov_df = pd.DataFrame({\n",
    "    \"ID\": np.arange(1, 1 + len(cov_pred)),\n",
    "    \"Prediction\": cov_pred\n",
    "})\n",
    "\n",
    "# HELOC: IDs start at 3501\n",
    "heloc_start = 3501\n",
    "heloc_df = pd.DataFrame({\n",
    "    \"ID\": np.arange(heloc_start, heloc_start + len(heloc_pred)),\n",
    "    \"Prediction\": heloc_pred\n",
    "})\n",
    "\n",
    "# HIGGS: IDs start at 4547\n",
    "higgs_start = 4547\n",
    "higgs_df = pd.DataFrame({\n",
    "    \"ID\": np.arange(higgs_start, higgs_start + len(higgs_pred)),\n",
    "    \"Prediction\": higgs_pred\n",
    "})\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Merge all into one CSV\n",
    "# -----------------------------\n",
    "submission = pd.concat([cov_df, heloc_df, higgs_df], ignore_index=True)\n",
    "\n",
    "print(\"\\nSubmission preview:\")\n",
    "print(submission.head())\n",
    "print(submission.tail())\n",
    "print(\"\\nTotal rows:\", len(submission))\n",
    "\n",
    "# Save\n",
    "submission_path = \"combined_submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\n✔ Saved unified submission to: {submission_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c3042-1005-4cb7-90ff-94589f008307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b3e97-0493-4836-8237-0cc363bf67d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a7bd7c-7b12-47f3-a8d0-aa00b15feb49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee991b17-bf21-489d-a991-52b70a843bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd66f50-1f6c-40a5-b78a-25f6c00f423b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31add7c7-559e-4721-98b4-86c0a409b637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54bb8cd-6c32-488d-85d1-db5907911e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40ba1d-216a-4236-99db-f2ad45988bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dc5bbb-72e9-4658-8999-bdc6d6571f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b023e-0e8b-4a99-b5c8-293a777789d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a189550-fa51-495b-b71e-813fa387fdf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c77ce1c-c265-46da-8287-1db07066bf06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
