{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a18909b-7808-4eff-afcd-1998eaaaaba4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "327a4248-8a00-4817-a6ad-870ca0250f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc08b52-dee1-4064-92a5-e762f40f5014",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3558e822-643d-4422-ba0a-c1404a435325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /home/ronlakeman/Applied Machine Learning/UvA-AML-2025/AppliedMachineLearning/Data\n",
      "Covertype train: ../Data/covtype_train.csv\n",
      "HELOC train: ../Data/heloc_train.csv\n",
      "HIGGS train: ../Data/higgs_train.csv\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = \"../Data\"  # Folder\n",
    "\n",
    "# Covertype\n",
    "COV_TRAIN = os.path.join(DATA_DIR, \"covtype_train.csv\")\n",
    "COV_TEST  = os.path.join(DATA_DIR, \"covtype_test.csv\")\n",
    "\n",
    "# HELOC\n",
    "HELOC_TRAIN = os.path.join(DATA_DIR, \"heloc_train.csv\")\n",
    "HELOC_TEST  = os.path.join(DATA_DIR, \"heloc_test.csv\")\n",
    "\n",
    "# HIGGS\n",
    "HIGGS_TRAIN = os.path.join(DATA_DIR, \"higgs_train.csv\")\n",
    "HIGGS_TEST  = os.path.join(DATA_DIR, \"higgs_test.csv\")\n",
    "\n",
    "# Sample submissions (optional)\n",
    "COV_SAMPLE_SUB   = os.path.join(DATA_DIR, \"covtype_test_submission.csv\")\n",
    "HELOC_SAMPLE_SUB = os.path.join(DATA_DIR, \"heloc_test_submission.csv\")\n",
    "HIGGS_SAMPLE_SUB = os.path.join(DATA_DIR, \"higgs_test_submission.csv\")\n",
    "\n",
    "print(\"Data directory:\", os.path.abspath(DATA_DIR))\n",
    "print(\"Covertype train:\", COV_TRAIN)\n",
    "print(\"HELOC train:\", HELOC_TRAIN)\n",
    "print(\"HIGGS train:\", HIGGS_TRAIN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e992831-6972-4b2b-a7ed-f83a7eec702c",
   "metadata": {},
   "source": [
    "# Load and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14aab1a1-ca0e-44d1-b4c6-bfc7c86316f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_covertype():\n",
    "    \"\"\"Load and preprocess the CoverType dataset.\n",
    "\n",
    "    Expects:\n",
    "    - training.csv with a column Cover_Type or CoverType as target.\n",
    "    - test.csv with the same feature columns (without target).\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(COV_TRAIN)\n",
    "    df_test  = pd.read_csv(COV_TEST)\n",
    "\n",
    "    # Target column can be 'Cover_Type' or 'CoverType'\n",
    "    if \"Cover_Type\" in df_train.columns:\n",
    "        y_col = \"Cover_Type\"\n",
    "    elif \"CoverType\" in df_train.columns:\n",
    "        y_col = \"CoverType\"\n",
    "    else:\n",
    "        raise ValueError(\"Could not find CoverType label column in covertype training.csv\")\n",
    "\n",
    "    y = df_train[y_col].values\n",
    "    X = df_train.drop(columns=[y_col])\n",
    "    X_test = df_test.copy()\n",
    "\n",
    "    # All numeric; scale with StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X.astype(np.float32))\n",
    "    X_test_scaled = scaler.transform(X_test.astype(np.float32))\n",
    "\n",
    "    return X_scaled, y, X_test_scaled\n",
    "\n",
    "\n",
    "def load_heloc():\n",
    "    \"\"\"Load and preprocess the HELOC dataset.\n",
    "\n",
    "    - training.csv with column 'RiskPerformance' (Good/Bad).\n",
    "    - test.csv with the same feature columns.\n",
    "    - Sentinel codes -7, -8, -9 are treated as missing and imputed.\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(HELOC_TRAIN)\n",
    "    df_test  = pd.read_csv(HELOC_TEST)\n",
    "\n",
    "    # Label: Good/Bad -> 0/1 (Bad = 1)\n",
    "    y = (df_train[\"RiskPerformance\"] == \"Bad\").astype(int).values\n",
    "    X = df_train.drop(columns=[\"RiskPerformance\"])\n",
    "    X_test = df_test.copy()\n",
    "\n",
    "    # Replace sentinel values with NaN\n",
    "    sentinel = [-7, -8, -9]\n",
    "    X = X.replace(sentinel, np.nan).astype(np.float32)\n",
    "    X_test = X_test.replace(sentinel, np.nan).astype(np.float32)\n",
    "\n",
    "    # Impute NaNs with train medians\n",
    "    medians = X.median()\n",
    "    X = X.fillna(medians)\n",
    "    X_test = X_test.fillna(medians)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_scaled, y, X_test_scaled\n",
    "\n",
    "\n",
    "def load_higgs():\n",
    "    \"\"\"Load and preprocess the HIGGS dataset.\n",
    "\n",
    "    - training.csv with columns: EventId, 30 features, Weight, Label (b/s or 0/1).\n",
    "    - test.csv with EventId and the 30 feature columns.\n",
    "\n",
    "    We treat -999.0 as missing and impute with medians.\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(HIGGS_TRAIN)\n",
    "    df_test  = pd.read_csv(HIGGS_TEST)\n",
    "\n",
    "    # Label may be given as 'Label' (b/s) or 'label' (0/1)\n",
    "    if \"Label\" in df_train.columns:\n",
    "        y_raw = df_train[\"Label\"]\n",
    "        y = (y_raw == \"s\").astype(int).values\n",
    "    elif \"label\" in df_train.columns:\n",
    "        y = df_train[\"label\"].astype(int).values\n",
    "    else:\n",
    "        raise ValueError(\"Could not find label column ('Label' or 'label') in HIGGS training.csv\")\n",
    "\n",
    "    # Sample weights (if available)\n",
    "    if \"Weight\" in df_train.columns:\n",
    "        w = df_train[\"Weight\"].values.astype(np.float32)\n",
    "    else:\n",
    "        w = np.ones(len(df_train), dtype=np.float32)\n",
    "\n",
    "    # Features: drop ID, Weight, label columns\n",
    "    drop_cols = [c for c in [\"EventId\", \"Weight\", \"Label\", \"label\"] if c in df_train.columns]\n",
    "    feature_cols = [c for c in df_train.columns if c not in drop_cols]\n",
    "    X = df_train[feature_cols].copy()\n",
    "    X_test = df_test[feature_cols].copy()\n",
    "\n",
    "    # Replace sentinel -999.0 with NaN\n",
    "    X = X.replace(-999.0, np.nan).astype(np.float32)\n",
    "    X_test = X_test.replace(-999.0, np.nan).astype(np.float32)\n",
    "\n",
    "    # Impute with medians\n",
    "    medians = X.median()\n",
    "    X = X.fillna(medians)\n",
    "    X_test = X_test.fillna(medians)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Event IDs (optional, useful for submissions)\n",
    "    event_id_train = df_train[\"EventId\"] if \"EventId\" in df_train.columns else None\n",
    "    event_id_test  = df_test[\"EventId\"] if \"EventId\" in df_test.columns else None\n",
    "\n",
    "    return X_scaled, y, w, X_test_scaled, event_id_train, event_id_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f3da34-0300-49b1-9c47-8ca086d457a9",
   "metadata": {},
   "source": [
    "# load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae97df4b-6641-4d0f-a251-7ed8a46531c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoverType: (58101, 54) (58101,)\n",
      "HELOC: (9413, 23) (9413,)\n",
      "HIGGS: (175000, 30) (175000,)\n"
     ]
    }
   ],
   "source": [
    "X_cov,   y_cov,   X_cov_test   = load_covertype()\n",
    "X_heloc, y_heloc, X_heloc_test = load_heloc()\n",
    "X_higgs, y_higgs, w_higgs, X_higgs_test, eid_tr, eid_te = load_higgs()\n",
    "\n",
    "print(\"CoverType:\", X_cov.shape, y_cov.shape)\n",
    "print(\"HELOC:\", X_heloc.shape, y_heloc.shape)\n",
    "print(\"HIGGS:\", X_higgs.shape, y_higgs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e56e926-be4c-4099-8360-c16c252aa71b",
   "metadata": {},
   "source": [
    "# Build a unified feature and label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab5d9d61-c162-4b96-bfde-51ec18104cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified X_all: (242514, 110) y_all: (242514,)\n"
     ]
    }
   ],
   "source": [
    "# Feature block sizes\n",
    "d_cov   = X_cov.shape[1]\n",
    "d_heloc = X_heloc.shape[1]\n",
    "d_higgs = X_higgs.shape[1]\n",
    "\n",
    "# Total unified feature length + 3 dataset-indicator features\n",
    "D_total = d_cov + d_heloc + d_higgs + 3\n",
    "\n",
    "def embed_block(X, dataset_idx):\n",
    "    \"\"\"\n",
    "    Embed X (n_samples, d_dataset) into unified feature space.\n",
    "\n",
    "    dataset_idx: 0 = CoverType, 1 = HELOC, 2 = HIGGS\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    Z = np.zeros((n, D_total), dtype=np.float32)\n",
    "\n",
    "    if dataset_idx == 0:          # CoverType block\n",
    "        Z[:, :d_cov] = X\n",
    "    elif dataset_idx == 1:        # HELOC block\n",
    "        Z[:, d_cov:d_cov + d_heloc] = X\n",
    "    elif dataset_idx == 2:        # HIGGS block\n",
    "        Z[:, d_cov + d_heloc:d_cov + d_heloc + d_higgs] = X\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset_idx, expected 0,1,2\")\n",
    "\n",
    "    # Dataset one-hot in last 3 positions\n",
    "    Z[:, D_total - 3 + dataset_idx] = 1.0\n",
    "\n",
    "    return Z\n",
    "\n",
    "\n",
    "# ----- Map labels into unified label space -----\n",
    "\n",
    "# CoverType: original labels (1..7) -> map to 0..6\n",
    "cov_unique = np.sort(np.unique(y_cov))\n",
    "cov_map = {v: i for i, v in enumerate(cov_unique)}\n",
    "y_cov_int = np.array([cov_map[v] for v in y_cov], dtype=np.int64)\n",
    "\n",
    "# HELOC: 0/1 -> shift by +7  (7, 8)\n",
    "y_heloc_int = y_heloc.astype(np.int64) + 7\n",
    "\n",
    "# HIGGS: 0/1 -> shift by +9  (9, 10)\n",
    "y_higgs_int = y_higgs.astype(np.int64) + 9\n",
    "\n",
    "# Embed features (train)\n",
    "X_cov_emb   = embed_block(X_cov,   dataset_idx=0)\n",
    "X_heloc_emb = embed_block(X_heloc, dataset_idx=1)\n",
    "X_higgs_emb = embed_block(X_higgs, dataset_idx=2)\n",
    "\n",
    "# Embed features (test) – needed for the submission step\n",
    "X_cov_test_emb   = embed_block(X_cov_test,   dataset_idx=0)\n",
    "X_heloc_test_emb = embed_block(X_heloc_test, dataset_idx=1)\n",
    "X_higgs_test_emb = embed_block(X_higgs_test, dataset_idx=2)\n",
    "\n",
    "# Concatenate everything (train)\n",
    "X_all = np.vstack([X_cov_emb, X_heloc_emb, X_higgs_emb])\n",
    "y_all = np.concatenate([y_cov_int, y_heloc_int, y_higgs_int])\n",
    "\n",
    "# Sample weights: CoverType & HELOC = 1, HIGGS uses its normalised weight\n",
    "w_cov   = np.ones_like(y_cov_int,   dtype=np.float32)\n",
    "w_heloc = np.ones_like(y_heloc_int, dtype=np.float32)\n",
    "w_higgs = np.ones_like(y_heloc_int, dtype=np.float32)\n",
    "\n",
    "sample_weight_all = np.concatenate([w_cov, w_heloc, w_higgs])\n",
    "\n",
    "print(\"Unified X_all:\", X_all.shape, \"y_all:\", y_all.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260707b2-4434-4425-a6c5-5e2b417d18a7",
   "metadata": {},
   "source": [
    "# Train/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f32e411b-6495-4c57-bc09-1e691b3e7a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (194011, 110) Val: (48503, 110)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val, w_train, w_val = train_test_split(\n",
    "    X_all,\n",
    "    y_all,\n",
    "    sample_weight_all,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=y_all,\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f34c51-f0f2-4d67-95a6-1c206002302d",
   "metadata": {},
   "source": [
    "# Train foundationmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105744d4-620b-4771-b3cf-deda7bb7e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [6, 8],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 1.0],\n",
    "}\n",
    "\n",
    "xgb_base = XGBClassifier(\n",
    "    random_state=SEED,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    n_jobs=-1,\n",
    "    verbosity=1,\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_base,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "print(\"Starting grid search...\")\n",
    "grid_search.fit(X_train, y_train, sample_weight=w_train)\n",
    "\n",
    "clf = grid_search.best_estimator_\n",
    "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV accuracy: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855f620e-bb03-4b27-9060-f1cf3f444ec3",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8d5a40f-f95a-45bd-a499-fc359eeb58bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoverType accuracy (on provided data): 0.9281\n",
      "CoverType: 0.9281423727646684\n",
      "HELOC accuracy (on provided data): 0.9194\n",
      "HELOC:    0.9193668331031553\n",
      "HIGGS accuracy (on provided data): 0.6872\n",
      "HIGGS:    0.6872057142857143\n",
      "HIGGS accuracy (on provided data): 0.6872\n",
      "HIGGS:    0.6872057142857143\n"
     ]
    }
   ],
   "source": [
    "def eval_dataset(X_emb, y_int, name):\n",
    "    y_pred_int = clf.predict(X_emb)\n",
    "    acc = accuracy_score(y_int, y_pred_int)\n",
    "    print(f\"{name} accuracy (on provided data): {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "print(\"CoverType:\", eval_dataset(X_cov_emb,   y_cov_int,   \"CoverType\"))\n",
    "print(\"HELOC:   \", eval_dataset(X_heloc_emb, y_heloc_int, \"HELOC\"))\n",
    "print(\"HIGGS:   \", eval_dataset(X_higgs_emb, y_higgs_int, \"HIGGS\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67c3b1-dfec-49e9-8feb-8ffd000dc0d3",
   "metadata": {},
   "source": [
    "# Generate submition file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90d32de7-4a01-4850-95ce-e23ae37ea537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating unified submission file...\n",
      "\n",
      "\n",
      "Submission preview:\n",
      "   ID  Prediction\n",
      "0   1           0\n",
      "1   2           0\n",
      "2   3           0\n",
      "3   4           0\n",
      "4   5           0\n",
      "          ID  Prediction\n",
      "79541  79542           9\n",
      "79542  79543           9\n",
      "79543  79544           9\n",
      "79544  79545           9\n",
      "79545  79546           9\n",
      "\n",
      "Total rows: 79546\n",
      "\n",
      "✔ Saved unified submission to: combined_submission.csv\n",
      "\n",
      "Submission preview:\n",
      "   ID  Prediction\n",
      "0   1           0\n",
      "1   2           0\n",
      "2   3           0\n",
      "3   4           0\n",
      "4   5           0\n",
      "          ID  Prediction\n",
      "79541  79542           9\n",
      "79542  79543           9\n",
      "79543  79544           9\n",
      "79544  79545           9\n",
      "79545  79546           9\n",
      "\n",
      "Total rows: 79546\n",
      "\n",
      "✔ Saved unified submission to: combined_submission.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating unified submission file...\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Embed test datasets\n",
    "# -----------------------------\n",
    "X_cov_test_emb  = embed_block(X_cov_test,  dataset_idx=0)\n",
    "X_heloc_test_emb = embed_block(X_heloc_test, dataset_idx=1)\n",
    "X_higgs_test_emb = embed_block(X_higgs_test, dataset_idx=2)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Predict each dataset\n",
    "# -----------------------------\n",
    "cov_pred   = clf.predict(X_cov_test_emb)\n",
    "heloc_pred = clf.predict(X_heloc_test_emb)\n",
    "higgs_pred = clf.predict(X_higgs_test_emb)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Assign correct ID ranges\n",
    "# -----------------------------\n",
    "# CoverType: IDs start at 1\n",
    "cov_df = pd.DataFrame({\n",
    "    \"ID\": np.arange(1, 1 + len(cov_pred)),\n",
    "    \"Prediction\": cov_pred\n",
    "})\n",
    "\n",
    "# HELOC: IDs start at 3501\n",
    "heloc_start = 3501\n",
    "heloc_df = pd.DataFrame({\n",
    "    \"ID\": np.arange(heloc_start, heloc_start + len(heloc_pred)),\n",
    "    \"Prediction\": heloc_pred\n",
    "})\n",
    "\n",
    "# HIGGS: IDs start at 4547\n",
    "higgs_start = 4547\n",
    "higgs_df = pd.DataFrame({\n",
    "    \"ID\": np.arange(higgs_start, higgs_start + len(higgs_pred)),\n",
    "    \"Prediction\": higgs_pred\n",
    "})\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Merge all into one CSV\n",
    "# -----------------------------\n",
    "submission = pd.concat([cov_df, heloc_df, higgs_df], ignore_index=True)\n",
    "\n",
    "print(\"\\nSubmission preview:\")\n",
    "print(submission.head())\n",
    "print(submission.tail())\n",
    "print(\"\\nTotal rows:\", len(submission))\n",
    "\n",
    "# Save\n",
    "submission_path = \"combined_submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\n✔ Saved unified submission to: {submission_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML_Project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
