{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a33fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31c4594a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covertype train: ../Data/covtype_train.csv\n",
      "HELOC train: ../Data/heloc_train.csv\n",
      "HIGGS train: ../Data/higgs_train.csv\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = \"../Data\"\n",
    "\n",
    "# Covertype\n",
    "COV_TRAIN = os.path.join(\"../Data\", \"covtype_train.csv\")\n",
    "COV_TEST  = os.path.join(DATA_DIR, \"covtype_test.csv\")\n",
    "\n",
    "# HELOC\n",
    "HELOC_TRAIN = os.path.join(DATA_DIR, \"heloc_train.csv\")\n",
    "HELOC_TEST  = os.path.join(DATA_DIR, \"heloc_test.csv\")\n",
    "\n",
    "# HIGGS\n",
    "HIGGS_TRAIN = os.path.join(DATA_DIR, \"higgs_train.csv\")\n",
    "HIGGS_TEST  = os.path.join(DATA_DIR, \"higgs_test.csv\")\n",
    "\n",
    "# Sample submissions (optional)\n",
    "COV_SAMPLE_SUB   = os.path.join(DATA_DIR, \"covtype_test_submission.csv\")\n",
    "HELOC_SAMPLE_SUB = os.path.join(DATA_DIR, \"heloc_test_submission.csv\")\n",
    "HIGGS_SAMPLE_SUB = os.path.join(DATA_DIR, \"higgs_test_submission.csv\")\n",
    "\n",
    "# print(\"Data directory:\", os.path.abspath(DATA_DIR))\n",
    "print(\"Covertype train:\", COV_TRAIN)\n",
    "print(\"HELOC train:\", HELOC_TRAIN)\n",
    "print(\"HIGGS train:\", HIGGS_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f12917b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    21297\n",
      "2    28248\n",
      "3     3607\n",
      "4      259\n",
      "5      932\n",
      "6     1706\n",
      "7     2052\n",
      "Name: count, dtype: int64\n",
      "CoverType: (58101, 54) (58101,)\n",
      "CoverType test: (3500, 54)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type31</th>\n",
       "      <th>Soil_Type32</th>\n",
       "      <th>Soil_Type33</th>\n",
       "      <th>Soil_Type34</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3351</td>\n",
       "      <td>206</td>\n",
       "      <td>27</td>\n",
       "      <td>726</td>\n",
       "      <td>124</td>\n",
       "      <td>3813</td>\n",
       "      <td>192</td>\n",
       "      <td>252</td>\n",
       "      <td>180</td>\n",
       "      <td>2271</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2732</td>\n",
       "      <td>129</td>\n",
       "      <td>7</td>\n",
       "      <td>212</td>\n",
       "      <td>1</td>\n",
       "      <td>1082</td>\n",
       "      <td>231</td>\n",
       "      <td>236</td>\n",
       "      <td>137</td>\n",
       "      <td>912</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2572</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>201</td>\n",
       "      <td>25</td>\n",
       "      <td>957</td>\n",
       "      <td>216</td>\n",
       "      <td>222</td>\n",
       "      <td>142</td>\n",
       "      <td>2191</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2824</td>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>417</td>\n",
       "      <td>39</td>\n",
       "      <td>3223</td>\n",
       "      <td>233</td>\n",
       "      <td>214</td>\n",
       "      <td>110</td>\n",
       "      <td>6478</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2529</td>\n",
       "      <td>84</td>\n",
       "      <td>5</td>\n",
       "      <td>120</td>\n",
       "      <td>9</td>\n",
       "      <td>1092</td>\n",
       "      <td>227</td>\n",
       "      <td>231</td>\n",
       "      <td>139</td>\n",
       "      <td>4983</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58096</th>\n",
       "      <td>3160</td>\n",
       "      <td>315</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1366</td>\n",
       "      <td>199</td>\n",
       "      <td>234</td>\n",
       "      <td>174</td>\n",
       "      <td>1129</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58097</th>\n",
       "      <td>2607</td>\n",
       "      <td>45</td>\n",
       "      <td>12</td>\n",
       "      <td>242</td>\n",
       "      <td>52</td>\n",
       "      <td>977</td>\n",
       "      <td>223</td>\n",
       "      <td>214</td>\n",
       "      <td>123</td>\n",
       "      <td>1342</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58098</th>\n",
       "      <td>2317</td>\n",
       "      <td>280</td>\n",
       "      <td>25</td>\n",
       "      <td>190</td>\n",
       "      <td>64</td>\n",
       "      <td>433</td>\n",
       "      <td>144</td>\n",
       "      <td>233</td>\n",
       "      <td>225</td>\n",
       "      <td>582</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58099</th>\n",
       "      <td>3183</td>\n",
       "      <td>89</td>\n",
       "      <td>17</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "      <td>3443</td>\n",
       "      <td>243</td>\n",
       "      <td>211</td>\n",
       "      <td>91</td>\n",
       "      <td>443</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58100</th>\n",
       "      <td>3177</td>\n",
       "      <td>171</td>\n",
       "      <td>22</td>\n",
       "      <td>108</td>\n",
       "      <td>39</td>\n",
       "      <td>722</td>\n",
       "      <td>229</td>\n",
       "      <td>244</td>\n",
       "      <td>135</td>\n",
       "      <td>2444</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58101 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0           3351     206     27                               726   \n",
       "1           2732     129      7                               212   \n",
       "2           2572      24      9                               201   \n",
       "3           2824      69     13                               417   \n",
       "4           2529      84      5                               120   \n",
       "...          ...     ...    ...                               ...   \n",
       "58096       3160     315      8                                 0   \n",
       "58097       2607      45     12                               242   \n",
       "58098       2317     280     25                               190   \n",
       "58099       3183      89     17                                60   \n",
       "58100       3177     171     22                               108   \n",
       "\n",
       "       Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                                 124                             3813   \n",
       "1                                   1                             1082   \n",
       "2                                  25                              957   \n",
       "3                                  39                             3223   \n",
       "4                                   9                             1092   \n",
       "...                               ...                              ...   \n",
       "58096                               0                             1366   \n",
       "58097                              52                              977   \n",
       "58098                              64                              433   \n",
       "58099                               8                             3443   \n",
       "58100                              39                              722   \n",
       "\n",
       "       Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0                192             252            180   \n",
       "1                231             236            137   \n",
       "2                216             222            142   \n",
       "3                233             214            110   \n",
       "4                227             231            139   \n",
       "...              ...             ...            ...   \n",
       "58096            199             234            174   \n",
       "58097            223             214            123   \n",
       "58098            144             233            225   \n",
       "58099            243             211             91   \n",
       "58100            229             244            135   \n",
       "\n",
       "       Horizontal_Distance_To_Fire_Points  ...  Soil_Type31  Soil_Type32  \\\n",
       "0                                    2271  ...            0            0   \n",
       "1                                     912  ...            0            0   \n",
       "2                                    2191  ...            0            0   \n",
       "3                                    6478  ...            0            0   \n",
       "4                                    4983  ...            0            0   \n",
       "...                                   ...  ...          ...          ...   \n",
       "58096                                1129  ...            0            0   \n",
       "58097                                1342  ...            0            0   \n",
       "58098                                 582  ...            0            0   \n",
       "58099                                 443  ...            0            0   \n",
       "58100                                2444  ...            0            0   \n",
       "\n",
       "       Soil_Type33  Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  \\\n",
       "0                0            0            0            0            0   \n",
       "1                0            0            0            0            0   \n",
       "2                0            0            0            0            0   \n",
       "3                0            0            0            0            0   \n",
       "4                0            0            0            0            0   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "58096            0            0            0            0            0   \n",
       "58097            0            0            0            0            0   \n",
       "58098            0            0            0            0            0   \n",
       "58099            0            0            0            0            0   \n",
       "58100            0            0            0            0            0   \n",
       "\n",
       "       Soil_Type38  Soil_Type39  Soil_Type40  \n",
       "0                1            0            0  \n",
       "1                0            0            0  \n",
       "2                0            0            0  \n",
       "3                0            0            0  \n",
       "4                0            0            0  \n",
       "...            ...          ...          ...  \n",
       "58096            0            0            0  \n",
       "58097            0            0            0  \n",
       "58098            0            0            0  \n",
       "58099            0            0            0  \n",
       "58100            0            0            0  \n",
       "\n",
       "[58101 rows x 54 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_covertype():\n",
    "    \"\"\"\n",
    "    scale_strategy: \"none\", \"standard\", \"minmax\", or \"robust\".\n",
    "    smote_ratio: target minority size as fraction of majority (None to skip).\n",
    "    undersample_ratio: keep this fraction of majority after SMOTE (None to skip).\n",
    "    Returns (X, y, X_test, scaler).\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(COV_TRAIN)\n",
    "    df_test = pd.read_csv(COV_TEST)\n",
    "\n",
    "    df_train.describe()\n",
    "\n",
    "    X_train = df_train.drop(columns=[\"Cover_Type\"])\n",
    "    X_test = df_test\n",
    "    y_train = df_train[\"Cover_Type\"].values\n",
    "\n",
    "    y_series = pd.Series(y_train)\n",
    "    print(y_series.value_counts().sort_index())\n",
    "\n",
    "    return X_train, y_train, X_test\n",
    "\n",
    "\n",
    "X_cov_train, y_cov_train,  X_cov_test = load_covertype()\n",
    "\n",
    "print(\"CoverType:\", X_cov_train.shape, y_cov_train.shape)\n",
    "print(\"CoverType test:\", X_cov_test.shape)\n",
    "\n",
    "\n",
    "display(X_cov_train)\n",
    "\n",
    "\n",
    "    # if smote_ratio is not None:\n",
    "    #     counts = pd.Series(y_train).value_counts()\n",
    "    #     max_class = counts.idxmax()\n",
    "    #     max_count = counts.max()\n",
    "    #     smote_strategy = {\n",
    "    #         cls: int(max_count * smote_ratio)\n",
    "    #         for cls in counts.index\n",
    "    #         if counts[cls] < max_count * smote_ratio\n",
    "    #     }\n",
    "    #     if smote_strategy:\n",
    "    #         smote = SMOTE(sampling_strategy=smote_strategy, random_state=42, k_neighbors=5)\n",
    "    #         X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # if undersample_ratio is not None:\n",
    "    #     counts = pd.Series(y_train).value_counts()\n",
    "    #     max_class = counts.idxmax()\n",
    "    #     max_count = counts.max()\n",
    "    #     target_majority = int(max_count * undersample_ratio)\n",
    "    #     if 0 < target_majority < max_count:\n",
    "    #         undersample = RandomUnderSampler(\n",
    "    #             sampling_strategy={max_class: target_majority}, random_state=42\n",
    "    #         )\n",
    "    #         X_train, y_train = undersample.fit_resample(X_train, y_train)\n",
    "\n",
    "    # # Handle the outliers\n",
    "    # scaler = None\n",
    "    # if scale_strategy == \"standard\":\n",
    "    #     scaler = StandardScaler()\n",
    "    # elif scale_strategy == \"minmax\":\n",
    "    #     scaler = MinMaxScaler()\n",
    "    # elif scale_strategy == \"robust\":\n",
    "    #     scaler = RobustScaler()\n",
    "\n",
    "\n",
    "    # X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X.columns, index=X_train.index)\n",
    "    # X_val = pd.DataFrame(scaler.transform(X_val), columns=X.columns, index=X_val.index)\n",
    "    # X_test = pd.DataFrame(scaler.transform(df_test), columns=X.columns)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# X_cov, y_cov, X_cov_test, cov_scaler = preprocess_covertype(\"standard\", 0.8, 0.65)\n",
    "# print(\"mean:\", X_cov.mean().mean(), \"std:\", X_cov.std().mean())\n",
    "\n",
    "# X_cov, y_cov, X_cov_test, cov_scaler = preprocess_covertype(\"minmax\", 0.8, 0.65)\n",
    "# print(\"mean:\", X_cov.mean().mean(), \"std:\", X_cov.std().mean())\n",
    "\n",
    "# X_cov, y_cov, X_cov_test, cov_scaler = preprocess_covertype(\"robust\", 0.8, 0.65)\n",
    "# print(\"mean:\", X_cov.mean().mean(), \"std:\", X_cov.std().mean())\n",
    "\n",
    "\n",
    "# Final scaling method to be used for modeling\n",
    "# scaling_method = \"standard\"\n",
    "# smote_ratio = 0.80\n",
    "# undersample_ratio = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d59b8e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELOC Train: (9413, 23) (9413,)\n",
      "HELOC Test: (1046, 23)\n",
      "0    4488\n",
      "1    4925\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExternalRiskEstimate</th>\n",
       "      <th>MSinceOldestTradeOpen</th>\n",
       "      <th>MSinceMostRecentTradeOpen</th>\n",
       "      <th>AverageMInFile</th>\n",
       "      <th>NumSatisfactoryTrades</th>\n",
       "      <th>NumTrades60Ever2DerogPubRec</th>\n",
       "      <th>NumTrades90Ever2DerogPubRec</th>\n",
       "      <th>PercentTradesNeverDelq</th>\n",
       "      <th>MSinceMostRecentDelq</th>\n",
       "      <th>MaxDelq2PublicRecLast12M</th>\n",
       "      <th>...</th>\n",
       "      <th>PercentInstallTrades</th>\n",
       "      <th>MSinceMostRecentInqexcl7days</th>\n",
       "      <th>NumInqLast6M</th>\n",
       "      <th>NumInqLast6Mexcl7days</th>\n",
       "      <th>NetFractionRevolvingBurden</th>\n",
       "      <th>NetFractionInstallBurden</th>\n",
       "      <th>NumRevolvingTradesWBalance</th>\n",
       "      <th>NumInstallTradesWBalance</th>\n",
       "      <th>NumBank2NatlTradesWHighUtilization</th>\n",
       "      <th>PercentTradesWBalance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>77.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9408</th>\n",
       "      <td>65.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9409</th>\n",
       "      <td>77.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9410</th>\n",
       "      <td>75.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9411</th>\n",
       "      <td>64.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9412</th>\n",
       "      <td>72.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9413 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ExternalRiskEstimate  MSinceOldestTradeOpen  MSinceMostRecentTradeOpen  \\\n",
       "0                     69.0                  148.0                        4.0   \n",
       "1                     77.0                  229.0                        3.0   \n",
       "2                     58.0                   46.0                        7.0   \n",
       "3                     72.0                  186.0                        6.0   \n",
       "4                     80.0                  226.0                        2.0   \n",
       "...                    ...                    ...                        ...   \n",
       "9408                  65.0                  115.0                       11.0   \n",
       "9409                  77.0                  437.0                        8.0   \n",
       "9410                  75.0                  140.0                        7.0   \n",
       "9411                  64.0                   92.0                        3.0   \n",
       "9412                  72.0                  186.0                        6.0   \n",
       "\n",
       "      AverageMInFile  NumSatisfactoryTrades  NumTrades60Ever2DerogPubRec  \\\n",
       "0               66.0                   41.0                          0.0   \n",
       "1              109.0                   23.0                          0.0   \n",
       "2               38.0                   13.0                          0.0   \n",
       "3               76.0                   20.0                          0.0   \n",
       "4               66.0                   35.0                          0.0   \n",
       "...              ...                    ...                          ...   \n",
       "9408            43.0                   19.0                          0.0   \n",
       "9409           115.0                   35.0                          0.0   \n",
       "9410            56.0                   21.0                          0.0   \n",
       "9411            35.0                   21.0                          2.0   \n",
       "9412            76.0                   20.0                          0.0   \n",
       "\n",
       "      NumTrades90Ever2DerogPubRec  PercentTradesNeverDelq  \\\n",
       "0                             0.0                   100.0   \n",
       "1                             0.0                   100.0   \n",
       "2                             0.0                    93.0   \n",
       "3                             0.0                    97.0   \n",
       "4                             0.0                   100.0   \n",
       "...                           ...                     ...   \n",
       "9408                          0.0                    90.0   \n",
       "9409                          0.0                   100.0   \n",
       "9410                          0.0                   100.0   \n",
       "9411                          2.0                    91.0   \n",
       "9412                          0.0                    97.0   \n",
       "\n",
       "      MSinceMostRecentDelq  MaxDelq2PublicRecLast12M  ...  \\\n",
       "0                     15.0                       7.0  ...   \n",
       "1                     15.0                       7.0  ...   \n",
       "2                      8.0                       4.0  ...   \n",
       "3                     15.0                       6.0  ...   \n",
       "4                     15.0                       7.0  ...   \n",
       "...                    ...                       ...  ...   \n",
       "9408                   1.0                       4.0  ...   \n",
       "9409                  15.0                       7.0  ...   \n",
       "9410                  15.0                       7.0  ...   \n",
       "9411                  33.0                       6.0  ...   \n",
       "9412                  15.0                       6.0  ...   \n",
       "\n",
       "      PercentInstallTrades  MSinceMostRecentInqexcl7days  NumInqLast6M  \\\n",
       "0                     10.0                           0.0           1.0   \n",
       "1                     35.0                           0.0           0.0   \n",
       "2                     50.0                           0.0           2.0   \n",
       "3                     33.0                           0.0           1.0   \n",
       "4                     47.0                           0.0           0.0   \n",
       "...                    ...                           ...           ...   \n",
       "9408                  50.0                           0.0           0.0   \n",
       "9409                  26.0                           1.0           3.0   \n",
       "9410                  27.0                           0.0           1.0   \n",
       "9411                  23.0                           0.0           1.0   \n",
       "9412                  33.0                           0.0           1.0   \n",
       "\n",
       "      NumInqLast6Mexcl7days  NetFractionRevolvingBurden  \\\n",
       "0                       1.0                        32.0   \n",
       "1                       0.0                        38.0   \n",
       "2                       2.0                        80.0   \n",
       "3                       1.0                        30.0   \n",
       "4                       0.0                         2.0   \n",
       "...                     ...                         ...   \n",
       "9408                    0.0                        52.0   \n",
       "9409                    3.0                        23.0   \n",
       "9410                    1.0                        20.0   \n",
       "9411                    1.0                         9.0   \n",
       "9412                    1.0                        30.0   \n",
       "\n",
       "      NetFractionInstallBurden  NumRevolvingTradesWBalance  \\\n",
       "0                         60.0                         7.0   \n",
       "1                         93.0                         4.0   \n",
       "2                         84.0                         5.0   \n",
       "3                         73.0                         3.0   \n",
       "4                         77.0                         5.0   \n",
       "...                        ...                         ...   \n",
       "9408                      77.0                         5.0   \n",
       "9409                      74.0                         6.0   \n",
       "9410                      63.0                         3.0   \n",
       "9411                      58.0                         3.0   \n",
       "9412                      73.0                         3.0   \n",
       "\n",
       "      NumInstallTradesWBalance  NumBank2NatlTradesWHighUtilization  \\\n",
       "0                          3.0                                 1.0   \n",
       "1                          3.0                                 1.0   \n",
       "2                          4.0                                 1.0   \n",
       "3                          2.0                                 1.0   \n",
       "4                          7.0                                 0.0   \n",
       "...                        ...                                 ...   \n",
       "9408                       6.0                                 0.0   \n",
       "9409                       2.0                                 0.0   \n",
       "9410                       2.0                                 1.0   \n",
       "9411                       4.0                                 0.0   \n",
       "9412                       2.0                                 1.0   \n",
       "\n",
       "      PercentTradesWBalance  \n",
       "0                      50.0  \n",
       "1                      58.0  \n",
       "2                      90.0  \n",
       "3                      67.0  \n",
       "4                      62.0  \n",
       "...                     ...  \n",
       "9408                   85.0  \n",
       "9409                   50.0  \n",
       "9410                   56.0  \n",
       "9411                   53.0  \n",
       "9412                   67.0  \n",
       "\n",
       "[9413 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_heloc():\n",
    "    \"\"\"Load and preprocess the HELOC dataset.\n",
    "\n",
    "    - training.csv with column 'RiskPerformance' (Good/Bad).\n",
    "    - test.csv with the same feature columns.\n",
    "    - Sentinel codes -7, -8, -9 are treated as missing and imputed.\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(HELOC_TRAIN)\n",
    "    df_test  = pd.read_csv(HELOC_TEST)\n",
    "\n",
    "    # Label: Good/Bad -> 0/1 (Bad = 1) \n",
    "    y = (df_train[\"RiskPerformance\"] == \"Bad\").astype(int).values\n",
    "    X = df_train.drop(columns=[\"RiskPerformance\"]).astype(np.float32)\n",
    "    X_test = df_test.copy().astype(np.float32)\n",
    "\n",
    "    # Replace sentinel values with NaN\n",
    "    sentinel = [-7, -8, -9]\n",
    "    X = X.replace(sentinel, np.nan)\n",
    "    X_test = X_test.replace(sentinel, np.nan)\n",
    "\n",
    "    # Impute NaNs with train medians\n",
    "    medians = X.median()\n",
    "    X = X.fillna(medians)\n",
    "    X_test = X_test.fillna(medians)\n",
    "\n",
    "    # display(X)\n",
    "    # display(X_test)\n",
    "\n",
    "    # scaler = StandardScaler()\n",
    "    # X_scaled = scaler.fit_transform(X)\n",
    "    # X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X, y, X_test\n",
    "\n",
    "X_heloc_train, y_heloc_train, X_heloc_test = load_heloc()\n",
    "\n",
    "print(\"HELOC Train:\", X_heloc_train.shape, y_heloc_train.shape)\n",
    "print(\"HELOC Test:\", X_heloc_test.shape)\n",
    "\n",
    "y_heloc_series = pd.Series(y_heloc_train)\n",
    "print(y_heloc_series.value_counts().sort_index())\n",
    "\n",
    "X_heloc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3e7c89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIGGS: (175000, 30) (175000,)\n",
      "175000\n"
     ]
    }
   ],
   "source": [
    "# from EDA.eda_covtype import X_train_enc\n",
    "\n",
    "\n",
    "def load_higgs():\n",
    "    \"\"\"Load and preprocess the HIGGS dataset.\n",
    "\n",
    "    - training.csv with columns: EventId, 30 features, Weight, Label (b/s or 0/1).\n",
    "    - test.csv with EventId and the 30 feature columns.\n",
    "\n",
    "    We treat -999.0 as missing and impute with medians.\n",
    "    \"\"\"\n",
    "    df_train = pd.read_csv(HIGGS_TRAIN)\n",
    "    df_test  = pd.read_csv(HIGGS_TEST)\n",
    "\n",
    "\n",
    "    y_train = df_train[\"Label\"]\n",
    "    y_train = (y_train == \"s\").astype(int).values\n",
    "\n",
    "    w_train = df_train[\"Weight\"].values.astype(np.float32)\n",
    " \n",
    "    # Features: drop ID, Weight, label columns\n",
    "    drop_cols = [c for c in [\"EventId\", \"Weight\", \"Label\", \"label\"] if c in df_train.columns]\n",
    "    feature_cols = [c for c in df_train.columns if c not in drop_cols]\n",
    "\n",
    "    X_train = df_train[feature_cols].copy()\n",
    "    X_test = df_test[feature_cols].copy()\n",
    "\n",
    "    return X_train, y_train, w_train, X_test \n",
    "\n",
    "\n",
    "    # # Replace sentinel -999.0 with NaN\n",
    "    # X_train = X_train.replace(-999.0, np.nan).astype(np.float32)\n",
    "    # X_test = X_test.replace(-999.0, np.nan).astype(np.float32)\n",
    "\n",
    "    # # Impute with medians\n",
    "    # medians = X_train.median()\n",
    "    # X_train = X_train.fillna(medians)\n",
    "    # X_test_train = X_test.fillna(medians)\n",
    "\n",
    "    # # scaler = StandardScaler()\n",
    "    # # X_scaled = scaler.fit_transform(X)\n",
    "    # # X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # # Event IDs (optional, useful for submissions)\n",
    "    # event_id_train = df_train[\"EventId\"] if \"EventId\" in df_train.columns else None\n",
    "    # event_id_test  = df_test[\"EventId\"] if \"EventId\" in df_test.columns else None\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "X_higgs_train, y_higgs_train, w_higgs_train, X_higgs_test = load_higgs()\n",
    "\n",
    "\n",
    "print(\"HIGGS:\", X_higgs_train.shape, y_higgs_train.shape)\n",
    "print(len(w_higgs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9c4a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Shapes of X ---\n",
      "X_cov shape:   (58101, 54)\n",
      "X_heloc shape: (9413, 23)\n",
      "X_higgs shape: (175000, 30)\n",
      "Rows total X:   242514\n",
      "\n",
      "==============================\n",
      "\n",
      "--- Shapes of y ---\n",
      "y_cov shape:   (58101,)\n",
      "y_heloc shape: (9413,)\n",
      "y_higgs shape: (175000,)\n",
      "Rows total y:   242514\n",
      "\n",
      "==============================\n",
      "\n",
      "X_total shape: (242514, 110)\n",
      "y_total shape: (242514,)\n",
      "w_total_shape: 242514\n"
     ]
    }
   ],
   "source": [
    "def embed_block(X, index, dimensions):\n",
    "    \"\"\"Place a dataset's features into its slice of the unified space.\n",
    "\n",
    "    dataset_idx: 0 = CoverType, 1 = HELOC, 2 = HIGGS\n",
    "    \"\"\"\n",
    "    # Number of rows in de X trainingset:\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "\n",
    "    embedded = np.zeros((n_samples, dimensions), dtype=np.float32)\n",
    "\n",
    "    def set_index(index):\n",
    "        if index == 0:\n",
    "            return 0, d_cov\n",
    "        elif index == 1:\n",
    "            return d_cov, d_cov + d_heloc\n",
    "        elif index == 2: \n",
    "            return d_cov + d_heloc, d_cov + d_heloc + d_higgs\n",
    "\n",
    "    start, end = set_index(index)\n",
    "    # print(start)\n",
    "    # print(end)\n",
    "\n",
    "    # fill in the dataset with the 110 dimensions which will later be placed on top of each other to create the full dataset\n",
    "    embedded[:, start:end] = X\n",
    "\n",
    "    # Dataset indicator in the final 3 positions\n",
    "    embedded[:, D_total - 3 + index] = 1\n",
    "    return embedded\n",
    "\n",
    "# Label the target columns to avoid collisions across datasets\n",
    "cov_unique = np.sort(np.unique(y_cov_train))\n",
    "cov_map = {v: i for i, v in enumerate(cov_unique)}\n",
    "y_cov_int = np.array([cov_map[v] for v in y_cov_train], dtype=np.int64)     # 0..6\n",
    "\n",
    "y_heloc_int = y_heloc_train.astype(np.int64) + 7                            # 7,8\n",
    "y_higgs_int = y_higgs_train.astype(np.int64) + 9                            # 9,10\n",
    "\n",
    "d_cov = X_cov_train.shape[1]\n",
    "d_heloc = X_heloc_train.shape[1]\n",
    "d_higgs = X_higgs_train.shape[1]\n",
    "\n",
    "D_total = d_cov + d_heloc + d_higgs + 3\n",
    "\n",
    "# Embed features (train)\n",
    "X_cov_emb   = embed_block(X_cov_train, 0, D_total)\n",
    "X_heloc_emb = embed_block(X_heloc_train, 1, D_total)\n",
    "X_higgs_emb = embed_block(X_higgs_train, 2, D_total)\n",
    "\n",
    "\n",
    "# Embed features (test) – needed for the submission step\n",
    "X_cov_test_emb   = embed_block(X_cov_test, 0, D_total)\n",
    "X_heloc_test_emb = embed_block(X_heloc_test, 1, D_total)\n",
    "X_higgs_test_emb = embed_block(X_higgs_test, 2, D_total)\n",
    "\n",
    "# # Stack the datasets on top of each other\n",
    "X_train = np.vstack([X_cov_emb, X_heloc_emb, X_higgs_emb])\n",
    "y_train = np.concatenate([y_cov_int, y_heloc_int, y_higgs_int])\n",
    "\n",
    "w_cov_train = np.ones_like(y_cov_train, dtype=np.float32)\n",
    "w_heloc_train = np.ones_like(y_heloc_train, dtype=np.float32)\n",
    "w_train = np.concatenate([w_cov_train, w_heloc_train, w_higgs_train])\n",
    "\n",
    "# Statistics of the dataset\n",
    "print(\"--- Shapes of X ---\")\n",
    "print(f\"X_cov shape:   {X_cov_train.shape}\")\n",
    "print(f\"X_heloc shape: {X_heloc_train.shape}\")\n",
    "print(f\"X_higgs shape: {X_higgs_train.shape}\")\n",
    "\n",
    "print(f\"Rows total X:   {X_cov_train.shape[0] + X_heloc_train.shape[0] + X_higgs_train.shape[0]}\")\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# Check the shape of all the y\n",
    "print(\"--- Shapes of y ---\")\n",
    "print(f\"y_cov shape:   {y_cov_train.shape}\")\n",
    "print(f\"y_heloc shape: {y_heloc_train.shape}\")\n",
    "print(f\"y_higgs shape: {y_higgs_train.shape}\")\n",
    "\n",
    "# Feature block sizes\n",
    "d_cov   = X_cov_train.shape[1]\n",
    "d_heloc = X_heloc_train.shape[1]\n",
    "d_higgs = X_higgs_train.shape[1]\n",
    "\n",
    "print(f\"Rows total y:   {y_cov_train.shape[0] + y_heloc_train.shape[0] + y_higgs_train.shape[0]}\")\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "\n",
    "print(f\"X_total shape: {X_train.shape}\")\n",
    "print(f\"y_total shape: {y_train.shape}\")\n",
    "print(f\"w_total_shape: {len(w_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f99ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_smote_strategy(y, smote_ratio):\n",
    "    counts = pd.Series(y).value_counts()\n",
    "    max_class = counts.idxmax()\n",
    "    max_count = counts.max()\n",
    "    smote_strategy = {\n",
    "        cls: int(max_count * smote_ratio)\n",
    "        for cls in counts.index\n",
    "        if counts[cls] < int(max_count * smote_ratio) \n",
    "    }\n",
    "    return smote_strategy, counts\n",
    "\n",
    "def get_scaler_options():\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping of scaler names to their objects.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"standard\": StandardScaler(), \n",
    "        \"minmax\": MinMaxScaler(), \n",
    "        \"robust\": RobustScaler(), \n",
    "        \"none\": None,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9bd280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Available CPU cores: {os.cpu_count()}\")\n",
    "\n",
    "\n",
    "# def train_model(model_name, X, y, imputer, smote_ratio, scaler_type, use_grid):\n",
    "    \n",
    "#     # 1. Select the scaler that we will be using for scaling the data\n",
    "#     scaler_map = get_scaler_options()\n",
    "#     selected_scaler = scaler_map.get(scaler_type.lower())\n",
    "#     print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "#     print(f\"Selected scalar: {selected_scaler}\")\n",
    "\n",
    "\n",
    "#     # 2. Configure the smote strategy that will be used for resampling\n",
    "#     smote_strategy, counts = config_smote_strategy(y, smote_ratio)\n",
    "#     print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "#     print(f\"y count before SMOTE: {counts}\")\n",
    "\n",
    "#     # 3. Configure folding strategy\n",
    "#     cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "\n",
    "#     # 4.Configure the model chosen by the user, if no grid search is chosen the following hyperparameters are chosen by default.\n",
    "#     if model_name == \"mlp\":\n",
    "#         classifier = MLPClassifier(\n",
    "#             hidden_layer_sizes=(256, 256),\n",
    "#             activation=\"relu\",\n",
    "#             solver=\"adam\",\n",
    "#             alpha=1e-4,\n",
    "#             batch_size=256,\n",
    "#             learning_rate_init=1e-3,\n",
    "#             max_iter=80,\n",
    "#             early_stopping=True,\n",
    "#             n_iter_no_change=5,\n",
    "#             random_state=SEED,\n",
    "#             verbose=False\n",
    "#         )\n",
    "#     elif model_name == \"xgb\":\n",
    "#         classifier = XGBClassifier(\n",
    "#             n_estimators=400,\n",
    "#             max_depth=6,\n",
    "#             learning_rate=0.05,\n",
    "#             subsample=0.8,\n",
    "#             colsample_bytree=0.8,\n",
    "#             objective=\"multi:softprob\",\n",
    "#             tree_method=\"hist\",\n",
    "#             random_state=SEED,\n",
    "#             n_jobs=-1\n",
    "#         )\n",
    "#     print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "#     print(f\"Chosen model: {model_name}\")\n",
    "\n",
    "#     # 5. Define the Imblearn Pipeline\n",
    "#     pipeline = ImbPipeline([\n",
    "#         ('imputer', SimpleImputer(strategy='median')),\n",
    "#         ('smote', SMOTE(sampling_strategy=smote_strategy, random_state=SEED)),\n",
    "#         ('scaler', selected_scaler), \n",
    "#         ('classifier', classifier)\n",
    "#     ])\n",
    "\n",
    "#     # 6. Handle Grid Search logic if grid search is chosen\n",
    "#     if use_grid:\n",
    "#         if model_name == \"mlp\":\n",
    "#             param_grid = {\n",
    "#                 \"classifier__hidden_layer_sizes\": [(128, 128), (256, 256)],\n",
    "#                 \"classifier__alpha\": [1e-4, 1e-3],\n",
    "#                 \"classifier__learning_rate_init\": [1e-3, 5e-4],\n",
    "#                 \"smote__k_neighbors\": [3, 5, 7]\n",
    "#             }\n",
    "#         elif model_name == \"xgb\":\n",
    "#             param_grid = {\n",
    "#                 \"classifier__n_estimators\": [300, 500],\n",
    "#                 \"classifier__max_depth\": [5, 7],\n",
    "#                 \"classifier__learning_rate\": [0.05, 0.1],\n",
    "#                 \"classifier__subsample\": [0.8, 1.0],\n",
    "#                 \"classifier__colsample_bytree\": [0.8, 1.0],\n",
    "#                 \"smote__k_neighbors\": [3, 5, 7]\n",
    "#             }\n",
    "#         # 'model' becomes the GridSearchCV object which wraps the pipeline\n",
    "#         model = GridSearchCV(\n",
    "#             estimator=pipeline, \n",
    "#             param_grid=param_grid,\n",
    "#             cv=cv_strategy, \n",
    "#             scoring=\"f1_weighted\", # F1-weighted is recommended for imbalance\n",
    "#             n_jobs=-1, \n",
    "#             verbose=1,\n",
    "#             refit=True, \n",
    "#         )\n",
    "#     else:\n",
    "#         # 'model' becomes the simple ImbPipeline object\n",
    "#         model = pipeline\n",
    "        \n",
    "#     print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "#     print(f\"Grid search: {use_grid}\")\n",
    "\n",
    "#     # model.fit(X_train, y_train, classifier__sample_weight=w_train if w_train is not None else None)\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#     return model, val_acc\n",
    "\n",
    "\n",
    "# use_grid = False\n",
    "# imputer = \"median\"\n",
    "# smote_ratio = 0.8\n",
    "# scalar_type = \"Standard\"\n",
    "\n",
    "# mlp_model, mlp_score = train_model(\"mlp\", X_train, y_train, imputer, smote_ratio, scalar_type, use_grid)\n",
    "\n",
    "# print(mlp_model, mlp_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a11c0e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, X, y, imputer, smote_ratio, scaler_type, use_grid, w=None, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Train a model with preprocessing pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str\n",
    "        Either \"mlp\" or \"xgb\"\n",
    "    X : array-like\n",
    "        Feature matrix\n",
    "    y : array-like\n",
    "        Target labels\n",
    "    imputer : str\n",
    "        Imputation strategy ('median', 'mean', etc.)\n",
    "    smote_ratio : float\n",
    "        Target ratio for SMOTE resampling (0-1)\n",
    "    scaler_type : str\n",
    "        Type of scaler: \"standard\", \"minmax\", \"robust\", or \"none\"\n",
    "    use_grid : bool\n",
    "        Whether to use GridSearchCV\n",
    "    w : array-like, optional\n",
    "        Sample weights for training (e.g., for HIGGS dataset)\n",
    "    test_size : float, optional\n",
    "        Proportion of data to use for validation (default 0.2)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : fitted model\n",
    "        Trained model (either ImbPipeline or GridSearchCV)\n",
    "    val_score : float\n",
    "        Validation F1-weighted score\n",
    "    \"\"\"\n",
    "    # 1. Handle data splitting based on whether we're using GridSearchCV\n",
    "    # GridSearchCV handles splits internally via CV, so we only split for non-grid case\n",
    "    if use_grid:\n",
    "        # For GridSearchCV: use all data, CV will handle splits\n",
    "        X_train, y_train = X, y\n",
    "        X_val, y_val = None, None\n",
    "    else:\n",
    "        # For non-grid: split for validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=SEED, stratify=y\n",
    "        )\n",
    "    \n",
    "    # 2. Select the scaler that we will be using for scaling the data\n",
    "    scaler_map = get_scaler_options()\n",
    "    selected_scaler = scaler_map.get(scaler_type.lower())\n",
    "    \n",
    "    # Handle None scaler - use 'passthrough' in pipeline\n",
    "    if selected_scaler is None:\n",
    "        scaler_step = 'passthrough'\n",
    "    else:\n",
    "        scaler_step = selected_scaler\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "    print(f\"Selected scaler: {scaler_type} ({selected_scaler})\")\n",
    "\n",
    "    # 3. Configure the smote strategy that will be used for resampling\n",
    "    smote_strategy, counts = config_smote_strategy(y_train, smote_ratio)\n",
    "    print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "    print(f\"y count before SMOTE:\\n{counts}\")\n",
    "\n",
    "    # 4. Configure folding strategy for GridSearchCV\n",
    "    cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "\n",
    "    # 5. Configure the model chosen by the user\n",
    "    if model_name == \"mlp\":\n",
    "        classifier = MLPClassifier(\n",
    "            hidden_layer_sizes=(256, 256),\n",
    "            activation=\"relu\",\n",
    "            solver=\"adam\",\n",
    "            alpha=1e-4,\n",
    "            batch_size=256,\n",
    "            learning_rate_init=1e-3,\n",
    "            max_iter=5,\n",
    "            early_stopping=True,\n",
    "            n_iter_no_change=5,\n",
    "            random_state=SEED,\n",
    "            verbose=True  # Enable to see training iterations\n",
    "        )\n",
    "    elif model_name == \"xgb\":\n",
    "        classifier = XGBClassifier(\n",
    "            n_estimators=400,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            objective=\"multi:softprob\",\n",
    "            tree_method=\"hist\",\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}. Use 'mlp' or 'xgb'\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "    print(f\"Chosen model: {model_name}\")\n",
    "\n",
    "    # 6. Define the Imblearn Pipeline\n",
    "    # Use the imputer parameter instead of hardcoding 'median'\n",
    "    pipeline = ImbPipeline([\n",
    "        ('imputer', SimpleImputer(strategy=imputer if isinstance(imputer, str) else 'median')),\n",
    "        ('smote', SMOTE(sampling_strategy=smote_strategy, random_state=SEED, k_neighbors=5)),\n",
    "        ('scaler', scaler_step), \n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "\n",
    "    # 7. Handle Grid Search logic if grid search is chosen\n",
    "    if use_grid:\n",
    "        if model_name == \"mlp\":\n",
    "            param_grid = {\n",
    "                \"classifier__hidden_layer_sizes\": [(128, 128), (256, 256)],\n",
    "                \"classifier__alpha\": [1e-4, 1e-3],\n",
    "                \"classifier__learning_rate_init\": [1e-3, 5e-4],\n",
    "                \"smote__k_neighbors\": [3, 5, 7]\n",
    "            }\n",
    "        elif model_name == \"xgb\":\n",
    "            param_grid = {\n",
    "                \"classifier__n_estimators\": [300, 500],\n",
    "                \"classifier__max_depth\": [5, 7],\n",
    "                \"classifier__learning_rate\": [0.05, 0.1],\n",
    "                \"classifier__subsample\": [0.8, 1.0],\n",
    "                \"classifier__colsample_bytree\": [0.8, 1.0],\n",
    "                \"smote__k_neighbors\": [3, 5, 7]\n",
    "            }\n",
    "        \n",
    "        # Calculate total number of fits for progress tracking\n",
    "        total_combinations = 1\n",
    "        for param_values in param_grid.values():\n",
    "            total_combinations *= len(param_values)\n",
    "        total_fits = total_combinations * cv_strategy.n_splits\n",
    "        print(f\"\\nGridSearchCV: Testing {total_combinations} parameter combinations\")\n",
    "        print(f\"  with {cv_strategy.n_splits}-fold CV = {total_fits} total fits\")\n",
    "        \n",
    "        # 'model' becomes the GridSearchCV object which wraps the pipeline\n",
    "        model = GridSearchCV(\n",
    "            estimator=pipeline, \n",
    "            param_grid=param_grid,\n",
    "            cv=cv_strategy, \n",
    "            scoring=\"f1_weighted\",  # F1-weighted is recommended for imbalance\n",
    "            n_jobs=-1, \n",
    "            verbose=2,  # Increased verbosity to see CV progress\n",
    "            refit=True, \n",
    "        )\n",
    "    else:\n",
    "        # 'model' becomes the simple ImbPipeline object\n",
    "        model = pipeline\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "    print(f\"Grid search: {use_grid}\")\n",
    "\n",
    "    # 8. Fit the model\n",
    "    fit_params = {}\n",
    "    if w is not None:\n",
    "        if use_grid:\n",
    "            # For GridSearchCV: use all weights\n",
    "            fit_params['classifier__sample_weight'] = w\n",
    "        else:\n",
    "            # For non-grid: split weights to match train/val split\n",
    "            _, _, w_train, _ = train_test_split(\n",
    "                X, w, test_size=test_size, random_state=SEED\n",
    "            )\n",
    "            fit_params['classifier__sample_weight'] = w_train\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"FITTING MODEL\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    if use_grid:\n",
    "        print(\"Starting GridSearchCV with cross-validation...\")\n",
    "        print(\"(Progress will show: [CV] fold scores for each parameter combination)\\n\")\n",
    "    else:\n",
    "        print(\"Training model on training set...\")\n",
    "        if model_name == \"mlp\":\n",
    "            print(\"(MLP will show iteration progress below)\\n\")\n",
    "        elif model_name == \"xgb\":\n",
    "            print(\"(XGBoost training in progress...)\\n\")\n",
    "    \n",
    "    model.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "    # 9. Evaluate model\n",
    "    if use_grid:\n",
    "        # For GridSearchCV: use the best CV score and show detailed results\n",
    "        val_score = model.best_score_\n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "        print(\"GRIDSEARCH RESULTS\")\n",
    "        print(\"=\"*30)\n",
    "        print(f\"Best CV F1-weighted score: {val_score:.4f}\")\n",
    "        print(f\"\\nBest parameters:\")\n",
    "        for param, value in model.best_params_.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "        \n",
    "        # Show CV scores for all parameter combinations\n",
    "        print(f\"\\nAll CV results (mean ± std across {cv_strategy.n_splits} folds):\")\n",
    "        results_df = pd.DataFrame(model.cv_results_)\n",
    "        # Sort by mean test score descending\n",
    "        results_df = results_df.sort_values('mean_test_score', ascending=False)\n",
    "        \n",
    "        # Show top 5 results\n",
    "        print(\"\\nTop 5 parameter combinations:\")\n",
    "        for idx, row in results_df.head(5).iterrows():\n",
    "            mean_score = row['mean_test_score']\n",
    "            std_score = row['std_test_score']\n",
    "            params = {k.replace('param_', ''): v for k, v in row.items() if k.startswith('param_')}\n",
    "            print(f\"  Score: {mean_score:.4f} (±{std_score:.4f}) | Params: {params}\")\n",
    "        \n",
    "        if len(results_df) > 5:\n",
    "            print(f\"  ... and {len(results_df) - 5} more combinations\")\n",
    "    else:\n",
    "        # For non-grid: evaluate on validation set\n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "        print(\"EVALUATING ON VALIDATION SET\")\n",
    "        print(\"=\"*30)\n",
    "        print(f\"Validation set size: {len(X_val)} samples\")\n",
    "        \n",
    "        y_val_pred = model.predict(X_val)\n",
    "        val_score = f1_score(y_val, y_val_pred, average='weighted', zero_division=0)\n",
    "        val_acc = accuracy_score(y_val, y_val_pred)\n",
    "        \n",
    "        print(f\"\\nValidation Metrics:\")\n",
    "        print(f\"  F1-weighted score: {val_score:.4f}\")\n",
    "        print(f\"  Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        # Show per-class metrics if not too many classes\n",
    "        unique_classes = len(np.unique(y_val))\n",
    "        if unique_classes <= 20:  # Only show if reasonable number of classes\n",
    "            print(f\"\\nPer-class F1 scores:\")\n",
    "            f1_per_class = f1_score(y_val, y_val_pred, average=None, zero_division=0)\n",
    "            for cls, f1 in enumerate(f1_per_class):\n",
    "                print(f\"  Class {cls}: {f1:.4f}\")\n",
    "\n",
    "    return model, val_score, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be3039d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "\n",
      "Selected scaler: Standard (StandardScaler())\n",
      "\n",
      "==============================\n",
      "\n",
      "y count before SMOTE:\n",
      "9     92171\n",
      "10    47829\n",
      "1     22598\n",
      "0     17037\n",
      "8      3940\n",
      "7      3590\n",
      "2      2886\n",
      "6      1642\n",
      "5      1365\n",
      "4       746\n",
      "3       207\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==============================\n",
      "\n",
      "Chosen model: mlp\n",
      "\n",
      "==============================\n",
      "\n",
      "Grid search: False\n",
      "\n",
      "==============================\n",
      "FITTING MODEL\n",
      "==============================\n",
      "Training model on training set...\n",
      "(MLP will show iteration progress below)\n",
      "\n",
      "Iteration 1, loss = 0.48829260\n",
      "Validation score: 0.834945\n",
      "Iteration 2, loss = 0.33690255\n",
      "Validation score: 0.865685\n",
      "Iteration 3, loss = 0.28937803\n",
      "Validation score: 0.881211\n",
      "Iteration 4, loss = 0.25974384\n",
      "Validation score: 0.887733\n",
      "Iteration 5, loss = 0.23752651\n",
      "Validation score: 0.901548\n",
      "\n",
      "==============================\n",
      "EVALUATING ON VALIDATION SET\n",
      "==============================\n",
      "Validation set size: 48503 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ronlakeman/Applied Machine Learning/UvA-AML-2025/AppliedMachineLearning/AML_Project_venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "  F1-weighted score: 0.8187\n",
      "  Accuracy: 0.8175\n",
      "\n",
      "Per-class F1 scores:\n",
      "  Class 0: 0.8065\n",
      "  Class 1: 0.8451\n",
      "  Class 2: 0.8392\n",
      "  Class 3: 0.6857\n",
      "  Class 4: 0.5678\n",
      "  Class 5: 0.7292\n",
      "  Class 6: 0.8496\n",
      "  Class 7: 0.6394\n",
      "  Class 8: 0.7154\n",
      "  Class 9: 0.8640\n",
      "  Class 10: 0.7498\n",
      "Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
      "                ('smote',\n",
      "                 SMOTE(random_state=42,\n",
      "                       sampling_strategy={0: 73736, 1: 73736, 2: 73736,\n",
      "                                          3: 73736, 4: 73736, 5: 73736,\n",
      "                                          6: 73736, 7: 73736, 8: 73736,\n",
      "                                          10: 73736})),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('classifier',\n",
      "                 MLPClassifier(batch_size=256, early_stopping=True,\n",
      "                               hidden_layer_sizes=(256, 256), max_iter=5,\n",
      "                               n_iter_no_change=5, random_state=42,\n",
      "                               verbose=True))]) 0.8186767815737364\n"
     ]
    }
   ],
   "source": [
    "use_grid = False\n",
    "imputer = \"median\"\n",
    "smote_ratio = 0.8\n",
    "scalar_type = \"Standard\"\n",
    "\n",
    "model, score, X_val, y_val = train_model(\"mlp\", X_train, y_train, imputer, smote_ratio, scalar_type, use_grid)\n",
    "\n",
    "\n",
    "print(model, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c79bdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoverType accuracy (on provided data): 0.8396\n",
      "CoverType: 0.8395724686322095\n",
      "HELOC accuracy (on provided data): 0.8014\n",
      "HELOC:    0.8014448103686391\n",
      "HIGGS accuracy (on provided data): 0.8269\n",
      "HIGGS:    0.8269371428571428\n",
      "\n",
      " === MLP Model Validation Metrics ===\n",
      "Accuracy: 0.817537059563326\n",
      "Precision: 0.8223239732367363\n",
      "Recall: 0.817537059563326\n",
      "F1 Score: 0.8186767815737364\n",
      "Confusion Matrix:\n",
      " [[ 3196   928     5     0    30     2    98     0     0     1     0]\n",
      " [  449  4845    92     0   174    75    15     0     0     0     0]\n",
      " [    0     9   634    11     4    63     0     0     0     0     0]\n",
      " [    0     0    10    36     0     6     0     0     0     0     0]\n",
      " [    0    24     4     0   157     1     0     0     0     0     0]\n",
      " [    0     8    45     6     2   280     0     0     0     0     0]\n",
      " [   21     2     0     0     0     0   387     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0   531   367     0     0]\n",
      " [    0     0     0     0     0     0     0   232   753     0     0]\n",
      " [    0     0     0     0     0     0     1     0     0 19594  3448]\n",
      " [    0     0     0     0     0     0     0     0     0  2717  9240]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.75      0.81      4260\n",
      "           1       0.83      0.86      0.85      5650\n",
      "           2       0.80      0.88      0.84       721\n",
      "           3       0.68      0.69      0.69        52\n",
      "           4       0.43      0.84      0.57       186\n",
      "           5       0.66      0.82      0.73       341\n",
      "           6       0.77      0.94      0.85       410\n",
      "           7       0.70      0.59      0.64       898\n",
      "           8       0.67      0.76      0.72       985\n",
      "           9       0.88      0.85      0.86     23043\n",
      "          10       0.73      0.77      0.75     11957\n",
      "\n",
      "    accuracy                           0.82     48503\n",
      "   macro avg       0.73      0.80      0.75     48503\n",
      "weighted avg       0.82      0.82      0.82     48503\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "def eval_dataset(X_emb, y_int, name):\n",
    "    y_pred_int = model.predict(X_emb)\n",
    "    acc = accuracy_score(y_int, y_pred_int)\n",
    "    print(f\"{name} accuracy (on provided data): {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "print(\"CoverType:\", eval_dataset(X_cov_emb,   y_cov_int, \"CoverType\"))\n",
    "print(\"HELOC:   \", eval_dataset(X_heloc_emb, y_heloc_int, \"HELOC\"))\n",
    "print(\"HIGGS:   \", eval_dataset(X_higgs_emb, y_higgs_int, \"HIGGS\"))\n",
    "\n",
    "def evaluate_model(model, X, y_true, average_type='weighted'):\n",
    "    \"\"\"\n",
    "    Compute and print evaluation metrics for a trained model.\n",
    "    \n",
    "    Parameters:\n",
    "        model: Trained model (must support predict method)\n",
    "        X: Features (numpy array or dataframe)\n",
    "        y_true: True labels (numpy array or series)\n",
    "        average_type: Averaging type for multiclass ('micro', 'macro', 'weighted')\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=average_type, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=average_type, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average_type, zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Precision:\", prec)\n",
    "    print(\"Recall:\", rec)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "# Example usage for validation set (weighted average for multiclass):\n",
    "print(\"\\n === MLP Model Validation Metrics ===\")\n",
    "evaluate_model(model, X_val, y_val, average_type='weighted')\n",
    "# If xgb_model un-commented above, you can also evaluate it:\n",
    "# print(\"=== XGB Model Validation Metrics ===\")\n",
    "# evaluate_model(xgb_model, X_val, y_val, average_type='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2db77392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoverType predictions (original labels 1-7): [1 2 3 4 5 6 7]\n",
      "HELOC predictions (original labels 0-1): [0 1]\n",
      "HIGGS predictions (original labels 0-1): [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Reverse mapping for predictions\n",
    "# CoverType: 0-6 -> 1-7\n",
    "inverse_cov_map = {v: k for k, v in cov_map.items()}\n",
    "\n",
    "# For MLP\n",
    "y_cov_test_pred = model.predict(X_cov_test_emb)\n",
    "y_heloc_test_pred = model.predict(X_heloc_test_emb)\n",
    "y_higgs_test_pred = model.predict(X_higgs_test_emb)\n",
    "\n",
    "# Predictions on test sets\n",
    "# y_cov_test_pred = clf.predict(X_cov_test_emb)\n",
    "# y_heloc_test_pred = clf.predict(X_heloc_test_emb)\n",
    "# y_higgs_test_pred = clf.predict(X_higgs_test_emb)\n",
    "\n",
    "\n",
    "# Convert predictions back to original label space\n",
    "# CoverType: 0-6 -> 1-7\n",
    "y_cov_test_pred_orig = np.array([inverse_cov_map[pred] for pred in y_cov_test_pred])\n",
    "\n",
    "# HELOC: 7, 8 -> 0, 1\n",
    "y_heloc_test_pred_orig = (y_heloc_test_pred - 7).astype(int)\n",
    "\n",
    "# HIGGS: 9, 10 -> 0, 1\n",
    "y_higgs_test_pred_orig = (y_higgs_test_pred - 9).astype(int)\n",
    "\n",
    "print(\"CoverType predictions (original labels 1-7):\", np.unique(y_cov_test_pred_orig))\n",
    "print(\"HELOC predictions (original labels 0-1):\", np.unique(y_heloc_test_pred_orig))\n",
    "print(\"HIGGS predictions (original labels 0-1):\", np.unique(y_higgs_test_pred_orig))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e54af23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission preview:\n",
      "   ID  Prediction\n",
      "0   1           1\n",
      "1   2           1\n",
      "2   3           1\n",
      "3   4           1\n",
      "4   5           1\n",
      "5   6           2\n",
      "6   7           1\n",
      "7   8           2\n",
      "8   9           2\n",
      "9  10           2\n",
      "...\n",
      "          ID  Prediction\n",
      "79536  79537           1\n",
      "79537  79538           1\n",
      "79538  79539           0\n",
      "79539  79540           1\n",
      "79540  79541           1\n",
      "79541  79542           1\n",
      "79542  79543           0\n",
      "79543  79544           0\n",
      "79544  79545           0\n",
      "79545  79546           0\n",
      "\n",
      "Total rows: 79546\n",
      "\n",
      "✓ Saved unified submission to: combined_submission.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate submission file with original label encoding\n",
    "\n",
    "# CoverType: IDs start at 1\n",
    "cov_df = pd.DataFrame({\n",
    "    \"ID\": np.arange(1, 1 + len(y_cov_test_pred_orig)),\n",
    "    \"Prediction\": y_cov_test_pred_orig\n",
    "})\n",
    "\n",
    "# HELOC: IDs start at 3501\n",
    "heloc_start = 3501\n",
    "heloc_df = pd.DataFrame({\n",
    "    \"ID\": np.arange(heloc_start, heloc_start + len(y_heloc_test_pred_orig)),\n",
    "    \"Prediction\": y_heloc_test_pred_orig\n",
    "})\n",
    "\n",
    "# HIGGS: IDs start at 4547\n",
    "higgs_start = 4547\n",
    "higgs_df = pd.DataFrame({\n",
    "    \"ID\": np.arange(higgs_start, higgs_start + len(y_higgs_test_pred_orig)),\n",
    "    \"Prediction\": y_higgs_test_pred_orig\n",
    "})\n",
    "\n",
    "# Merge all into one CSV\n",
    "submission = pd.concat([cov_df, heloc_df, higgs_df], ignore_index=True)\n",
    "\n",
    "print(\"\\nSubmission preview:\")\n",
    "print(submission.head(10))\n",
    "print(\"...\")\n",
    "print(submission.tail(10))\n",
    "print(f\"\\nTotal rows: {len(submission)}\")\n",
    "\n",
    "# Save\n",
    "submission_path = \"combined_submission.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\n✓ Saved unified submission to: {submission_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML_Project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
